{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "    \n",
    "#Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Path to your merged dataset pickle (from the previous step - see creating_merged_dataset.py)\n",
    "data_dir = '/path/to/your/dataset'\n",
    "merged_pickle = os.path.join(data_dir, \"merged_dataset_gpu1.pkl\")\n",
    "\n",
    "#Load the merged DataFrame\n",
    "with open(merged_pickle, \"rb\") as f:\n",
    "    merged_df = pickle.load(f)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Add binary column for \"No Finding\" presence\n",
    "merged_df[\"is_no_finding\"] = merged_df[\"Finding Labels\"].apply(lambda x: \"No Finding\" in x)\n",
    "\n",
    "#Split: 80% train+val, 20% test (stratified on No Finding)\n",
    "train_val_df, test_df = train_test_split(\n",
    "    merged_df,\n",
    "    test_size=0.2,\n",
    "    random_state=123,\n",
    "    stratify=merged_df[\"is_no_finding\"]\n",
    ")\n",
    "\n",
    "#From train+val, split into 62.5% train, 37.5% val\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df,\n",
    "    test_size=0.375,  # 0.375 of 80% = 30%\n",
    "    random_state=123,\n",
    "    stratify=train_val_df[\"is_no_finding\"]\n",
    ")\n",
    "\n",
    "#1) Define a transform to keep the image in RGB and convert to a tensor [0..1]\n",
    "transform = T.Compose([\n",
    "    T.ToTensor()  # shape will be [3, H, W] for RGB\n",
    "])\n",
    "\n",
    "#2) Initialise accumulators for each of the 3 channels\n",
    "sum_pixels = torch.zeros(3)\n",
    "sum_sq_pixels = torch.zeros(3)\n",
    "n_pixels = 0\n",
    "\n",
    "#3) Loop over each row in merged_df (our 3,000 images)\n",
    "for _, row in merged_df.iterrows():\n",
    "    img_path = row[\"file_path\"]\n",
    "    img = Image.open(img_path).convert(\"RGB\")  # ensure 3-channel\n",
    "    \n",
    "    if not isinstance(img, Image.Image):\n",
    "        img = Image.fromarray(img)  # Convert ndarray to PIL Image if needed\n",
    "    \n",
    "    assert isinstance(img, Image.Image), f\"Expected PIL.Image.Image, got {type(img)}\"\n",
    "    \n",
    "    if not Path(img_path).is_file():\n",
    "        print(f\"Skipping invalid image path: {img_path}\")\n",
    "        continue  # Skip this one\n",
    "\n",
    "    try:\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        assert isinstance(img, Image.Image), f\"Expected PIL.Image.Image, got {type(img)}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to open image at {img_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Convert image to a tensor in [0..1], shape [3, H, W]\n",
    "    tensor_img = transform(img)  \n",
    "\n",
    "    # tensor_img has shape (3, H, W). Let's get H and W:\n",
    "    _, H, W = tensor_img.shape\n",
    "    n_pixels += (H * W)  # for each channel, we'll have H*W pixels\n",
    "\n",
    "    # Sum of pixel values per channel -> shape [3]\n",
    "    sum_pixels += tensor_img.sum(dim=[1, 2])\n",
    "    # Sum of squared pixel values per channel -> shape [3]\n",
    "    sum_sq_pixels += (tensor_img ** 2).sum(dim=[1, 2])\n",
    "\n",
    "#4) Compute mean and std for each channel\n",
    "mean = sum_pixels / n_pixels\n",
    "variance = (sum_sq_pixels / n_pixels) - (mean ** 2)\n",
    "std = torch.sqrt(variance)\n",
    "\n",
    "transform_train = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2),  \n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=mean.tolist(), std=std.tolist())\n",
    "])\n",
    "\n",
    "transform_val_test = T.Compose([\n",
    "    T.Resize((224, 224)),  \n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=mean.tolist(), std=std.tolist())\n",
    "])\n",
    "\n",
    "##########\n",
    "class CXRDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        \"\"\"\n",
    "        df: DataFrame with columns ['file_path', 'Finding Labels']\n",
    "        transform: torchvision transforms for preprocessing\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = row[\"file_path\"]\n",
    "        label_str = row[\"Finding Labels\"]\n",
    "\n",
    "        # Open grayscale image and convert to RGB for VGG input\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Binary label: 1 = any disease, 0 = No Finding\n",
    "        binary_label = 0.0 if label_str == \"No Finding\" else 1.0\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(binary_label, dtype=torch.float)\n",
    "\n",
    "train_dataset = CXRDataset(train_df, transform=transform_train)\n",
    "val_dataset   = CXRDataset(val_df, transform=transform_val_test)\n",
    "test_dataset  = CXRDataset(test_df, transform=transform_val_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "#####MODEL#####\n",
    "\n",
    "import sys\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "#Hyperparameters\n",
    "num_epochs = 25\n",
    "learning_rate = 1e-6\n",
    "weight_decay = 1e-1\n",
    "\n",
    "#Load the pre-trained VGG16 model\n",
    "model = models.vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "\n",
    "#Unfreeze all layers initially\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "#Freezing the first block of the VGG16 model (convolutional layers)\n",
    "for param in model.features[:5].parameters():  # The first block has 10 layers\n",
    "    param.requires_grad = False\n",
    "    \n",
    "#Convert model.features to a list, add Dropout, and make a new Sequential\n",
    "model.features = nn.Sequential(\n",
    "    *list(model.features.children()), \n",
    "    nn.Dropout(p=0.5)  # Add dropout here\n",
    ")\n",
    "#Replacing final layer with binary output\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(25088, 4096),\n",
    "    nn.BatchNorm1d(4096),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),            #First dropout\n",
    "\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.BatchNorm1d(4096),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),            #Second dropout \n",
    "\n",
    "    nn.Linear(4096, 1)            #Final binary output layer\n",
    ")\n",
    "\n",
    "# Move model to GPU(s)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "#Optimiser and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,         #Optimiser to adjust the learning rate\n",
    "    mode='min',        #Minimise the validation loss\n",
    "    factor=0.5,        #Multiply LR by 0.5 when reducing\n",
    "    patience=2       #Wait for 3 epochs with no improvement before reducing LR\n",
    ")\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Evaluation Function\n",
    "def evaluate_and_plot(model, dataloader, device, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, binary_label in dataloader:\n",
    "            images = images.to(device)\n",
    "            binary_label = binary_label.to(device).unsqueeze(1)\n",
    "\n",
    "            outputs = model(images)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).int()\n",
    "\n",
    "            loss = criterion(outputs, binary_label)\n",
    "            total_loss += loss.item()\n",
    "            correct += (preds == binary_label.bool()).sum().item()\n",
    "            total += binary_label.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(binary_label.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    #Metrics\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "    auc_score = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    #Confusion matrix to compute FPR and FNR\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    #Calculate FPR and FNR\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "\n",
    "    return avg_loss, accuracy, precision, recall, f1, auc_score, fpr, fnr\n",
    "\n",
    "#Metric lists to store data across epochs\n",
    "train_losses, train_accuracies = [], []\n",
    "val_losses, val_accuracies = [], []\n",
    "val_precisions, val_recalls, val_f1s, val_aucs = [], [], [], []\n",
    "val_fprs, val_fnrs = [], []\n",
    "best_val_accuracy = 0.0  #Initialse to the worst possible accuracy (0%)\n",
    "\n",
    "#####Training loop\n",
    "\n",
    "early_stop_patience = 5\n",
    "epochs_no_improve = 0\n",
    "best_val_loss = float('inf')\n",
    "best_val_accuracy = 0.0  \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    correct_train, total_train = 0, 0\n",
    "\n",
    "    for images, binary_label in train_loader:\n",
    "        images = images.to(device)\n",
    "        binary_label = binary_label.to(device).unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, binary_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        preds = torch.sigmoid(outputs) > 0.5\n",
    "        correct_train += (preds == binary_label.bool()).sum().item()\n",
    "        total_train += binary_label.size(0)\n",
    "\n",
    "    #Compute training metrics\n",
    "    avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "    train_accuracy = correct_train / total_train\n",
    "\n",
    "    #Validation\n",
    "    val_loss, val_acc, val_prec, val_rec, val_f1, val_auc, val_fpr, val_fnr = evaluate_and_plot(model, val_loader, device, criterion)\n",
    "\n",
    "    #Append metrics\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    val_precisions.append(val_prec)\n",
    "    val_recalls.append(val_rec)\n",
    "    val_f1s.append(val_f1)\n",
    "    val_aucs.append(val_auc)\n",
    "    val_fprs.append(val_fpr)\n",
    "    val_fnrs.append(val_fnr)\n",
    "\n",
    "    #Learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val Recall: {val_rec:.4f}\")\n",
    "\n",
    "    #Early Stopping Check (still uses val_loss)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement in val_loss for {epochs_no_improve} epoch(s).\")\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    #Save Best Model Based on Accuracy\n",
    "    if val_acc > best_val_accuracy:\n",
    "        best_val_accuracy = val_acc\n",
    "        torch.save(model.state_dict(), 'best_vggfull3_accuracy.pth')\n",
    "        print(f\"Model saved at epoch {epoch+1} with val_acc: {val_acc:.4f}\")\n",
    "\n",
    "#Save final epoch metrics to CSV\n",
    "final_metrics = {\n",
    "    'final_epoch': num_epochs,\n",
    "    'final_train_loss': train_losses[-1] if len(train_losses) > 0 else None,\n",
    "    'final_train_acc': train_accuracies[-1] if len(train_accuracies) > 0 else None,\n",
    "    'final_val_loss': val_losses[-1] if len(val_losses) > 0 else None,\n",
    "    'final_val_acc': val_accuracies[-1] if len(val_accuracies) > 0 else None,\n",
    "    'final_val_precision': val_precisions[-1] if len(val_precisions) > 0 else None,\n",
    "    'final_val_recall': val_recalls[-1] if len(val_recalls) > 0 else None,\n",
    "    'final_val_f1': val_f1s[-1] if len(val_f1s) > 0 else None,\n",
    "    'final_val_auc': val_aucs[-1] if len(val_aucs) > 0 else None,\n",
    "    'final_val_fpr': val_fprs[-1] if len(val_fprs) > 0 else None,\n",
    "    'final_val_fnr': val_fnrs[-1] if len(val_fnrs) > 0 else None\n",
    "}\n",
    "\n",
    "#Save the metrics to CSV\n",
    "df = pd.DataFrame([final_metrics])\n",
    "df.to_csv(\"vggfull3.csv\", index=False)\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "#Accuracy Plot\n",
    "plt.figure() \n",
    "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(val_accuracies, label=\"Val Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy over Epochs\")\n",
    "plt.legend()\n",
    "plt.gca().xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "plt.savefig('accuracy_vggfull3_plt.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Loss Plot\n",
    "plt.figure() \n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.legend()\n",
    "plt.gca().xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "plt.savefig('loss_vggfull3_plt.png')\n",
    "plt.show()\n",
    "\n",
    "###Predicting on Test Set \n",
    "\n",
    "#Rebuild the same model architecture\n",
    "model = models.vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "\n",
    "#Unfreeze all layers initially\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "#Freeze first block again\n",
    "for param in model.features[:5].parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#Add dropout after features\n",
    "model.features = nn.Sequential(\n",
    "    *list(model.features.children()),\n",
    "    nn.Dropout(p=0.5)\n",
    ")\n",
    "\n",
    "#Rebuild classifier with BatchNorm and Dropout\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(25088, 4096),\n",
    "    nn.BatchNorm1d(4096),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(0.5),\n",
    "\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.BatchNorm1d(4096),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(0.5),\n",
    "\n",
    "    nn.Linear(4096, 1)\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_vggfull3_accuracy.pth\"))\n",
    "model = model.to(device)\n",
    "model.eval()  \n",
    "\n",
    "all_preds = []\n",
    "all_probs = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, binary_label in test_loader:\n",
    "        images = images.to(device)\n",
    "        binary_label = binary_label.to(device).unsqueeze(1)\n",
    "\n",
    "        outputs = model(images)\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = (probs > 0.5).int()\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "        all_labels.extend(binary_label.cpu().numpy())\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "#Confusion matrix to compute FPR and FNR\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "#Calculate FPR and FNR\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "\n",
    "#Confusion Matrix\n",
    "plt.figure() \n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[\"No Disease\", \"Disease\"],\n",
    "            yticklabels=[\"No Disease\", \"Disease\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig('confusion_vggfull3.png')\n",
    "plt.show()\n",
    "\n",
    "#ROC Curve\n",
    "plt.figure() \n",
    "fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc:.4f}\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.savefig('roc_vggfull3.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(\"Test Precision:\", precision)\n",
    "print(\"Test Recall:\", recall)\n",
    "print(\"Test F1 Score:\", f1)\n",
    "print(\"Test AUC:\", auc)\n",
    "print(\"False Positive Rate (FPR):\", fpr)\n",
    "print(\"False Negative Rate (FNR):\", fnr)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "df_metrics = pd.DataFrame({\n",
    "    'accuracy': [accuracy],\n",
    "    'precision': [precision],\n",
    "    'recall': [recall],\n",
    "    'f1_score': [f1],\n",
    "    'auc': [auc],\n",
    "    'fnr': [fnr],\n",
    "})\n",
    "\n",
    "df_metrics.to_csv(\"vggfull3_metrics.csv\", index=False)\n",
    "\n",
    "metrics_dict = {\n",
    "    'train_loss': train_losses,\n",
    "    'train_accuracy': train_accuracies,\n",
    "    'val_loss': val_losses,\n",
    "    'val_accuracy': val_accuracies,\n",
    "    'val_precision': val_precisions,\n",
    "    'val_recall': val_recalls,\n",
    "    'val_f1': val_f1s,\n",
    "    'val_auc': val_aucs,\n",
    "    'val_fnr': val_fnrs\n",
    "}\n",
    "\n",
    "#Step 2: Convert to a DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_dict)\n",
    "\n",
    "#Step 3: Save to CSV\n",
    "metrics_df.to_csv('training_metrics_vggfull3.csv', index_label='epoch')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
